{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRztzExrIX3O"
      },
      "source": [
        "# Hallucination Detection using Entropy Metrics\n",
        "\n",
        "This notebook benchmark the effectiveness of entropy and LLM as judge for detecting hallucinations in LLM responses using the HaluEval dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started\n",
        "Installs and imports necessary libraries"
      ],
      "metadata": {
        "id": "85NMrlTz0nOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsYwHp4IIX3P"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -qqq datasets litellm torch numpy pandas scikit-learn tqdm vllm model2vec together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJL3ocGSNMxw"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/klara-research/klarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaZYxvwgbD4Y"
      },
      "outputs": [],
      "source": [
        "cd klarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bavgPyDMbI44"
      },
      "outputs": [],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dQhjLVVIX3Q"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import litellm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from model2vec import StaticModel\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_curve, precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import math\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Dict, Tuple\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "from klarity.core.analyzer import EntropyAnalyzer\n",
        "from klarity.estimator import UncertaintyEstimator\n",
        "from klarity.models import TokenInfo\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F9NXzrz5bW5r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the API key\n",
        "os.environ[\"TOGETHERAI_API_KEY\"] = '..'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J02-AH6XIX3Q"
      },
      "source": [
        "# Get HuggingFace dataset\n",
        "Download hallucinations datasets from HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "louZSYnPIX3Q"
      },
      "outputs": [],
      "source": [
        "# Load the HaluEval dataset\n",
        "def get_halueval_dataset(split_name: str = \"qa\"):\n",
        "    \"\"\"\n",
        "    Load the HaluEval dataset.\n",
        "\n",
        "    Args:\n",
        "        split_name (str): The split name of the dataset. Default is \"qa\".\n",
        "\n",
        "    Returns:\n",
        "        datasets.Dataset: The loaded dataset.\n",
        "    \"\"\"\n",
        "    dataset = datasets.load_dataset(\"notrichardren/HaluEval\", split_name)\n",
        "    print(f\"Dataset size: {len(dataset['train'])}\")\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_AMnnk1TIX3R"
      },
      "outputs": [],
      "source": [
        "def get_trutfulqa_dataset(split_name: str = \"multiple_choice\"):\n",
        "    \"\"\"\n",
        "    Load the TrutfulQA dataset.\n",
        "\n",
        "    Args:\n",
        "        split_name (str): The split name of the dataset. Default is \"validation\".\n",
        "\n",
        "    Returns:\n",
        "        datasets.Dataset: The loaded dataset.\n",
        "    \"\"\"\n",
        "    dataset = datasets.load_dataset(\"truthfulqa/truthful_qa\", split_name)\n",
        "    print(f\"Dataset size: {len(dataset['validation'])}\")\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnVflKsaIX3R"
      },
      "outputs": [],
      "source": [
        "# Sample the top 100 rows of the dataset\n",
        "ds = get_trutfulqa_dataset()\n",
        "ds = ds['validation']\n",
        "len(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5enuOBoxIX3R"
      },
      "source": [
        "# Initialize Klarity\n",
        "Uncertainty Estimator to compute entropy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w9Qe4M7IX3R"
      },
      "outputs": [],
      "source": [
        "entropy_analyzer = EntropyAnalyzer()\n",
        "uncertainty_estimator = UncertaintyEstimator(top_k=5, analyzer=entropy_analyzer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2IofTIeIX3R"
      },
      "source": [
        "# Instantiate a vllm model\n",
        "This will be the evaluated model -  judge model is the one that is gonna review the answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYoqR9ewIX3R"
      },
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "judge_model_name = \"together_ai/Qwen/Qwen2.5-7B-Instruct-Turbo\"\n",
        "llm = LLM(model=model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SM-W0HJIX3S"
      },
      "source": [
        "# Instantiate the sampling params.\n",
        "We want to get the logprobs of the top-k tokens to compute entropy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VWppHqa5IX3S"
      },
      "outputs": [],
      "source": [
        "sampling_params = SamplingParams(\n",
        "    max_tokens=128,\n",
        "    temperature=0.0,\n",
        "    logprobs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebDgPpunIX3S"
      },
      "source": [
        "# VLLM response data\n",
        "\n",
        "This cell defines a function `get_vllm_response` that generates a response using a vLLM model\n",
        "and calculates entropy metrics for the generated text. The function takes four parameters:\n",
        " - llm: A vLLM model instance\n",
        " - tokenizer: The tokenizer for the model\n",
        " - sampling_params: Parameters for text generation\n",
        " - text: The input prompt\n",
        "\n",
        " The function returns a tuple containing:\n",
        "1. The generated text\n",
        "2. The raw output from the model\n",
        "3. The mean entropy of the generated tokens\n",
        "4. The mean semantic entropy of the generated tokens\n",
        "\n",
        "This function is used to analyze the uncertainty and potential hallucinations\n",
        "in the model's responses by leveraging entropy-based metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wYqw2YdrIX3S"
      },
      "outputs": [],
      "source": [
        "def get_vllm_response(\n",
        "    llm: LLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    sampling_params: SamplingParams,\n",
        "    text: str,\n",
        "    dataset_name: str = \"halueval\"\n",
        ") -> Tuple[str, Dict, float, float]:\n",
        "    \"\"\"\n",
        "    Get response using vLLM with entropy metrics.\n",
        "\n",
        "    Args:\n",
        "        llm: The vllm instance\n",
        "        tokenizer: The tokenizer of the model\n",
        "        sampling_params: The sampling parameters for response generation\n",
        "        text: Input text to get response for\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - Model's response text\n",
        "        - Raw output from model\n",
        "        - Mean entropy\n",
        "        - Mean semantic entropy\n",
        "    \"\"\"\n",
        "    mean_entropy = []\n",
        "    mean_semantic_entropy = []\n",
        "    if dataset_name == \"halueval\":\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"\"\"\\\n",
        "                You are a question answering assistant. Use the provided context to answer the question.\\\n",
        "                Respond with only the answer and no other context\"\"\"\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": text}]\n",
        "    else:\n",
        "         messages = [\n",
        "            {\"role\": \"system\", \"content\": \"\"\"\\\n",
        "                You are a MCQ answering agent. Answer the following question by picking the correct choice. You will\n",
        "                respond with just the correct MCQ choice id i.e. A, B, C, D, ....\n",
        "                \"\"\"\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": text}]\n",
        "    input_text=tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    vllm_response = llm.generate(input_text, sampling_params)\n",
        "    answer = vllm_response[0].outputs[0].text\n",
        "    analysis_results = uncertainty_estimator.analyze_generation(vllm_response[0])\n",
        "    for token_metric in analysis_results.token_metrics:\n",
        "        mean_entropy.append(token_metric.raw_entropy)\n",
        "        mean_semantic_entropy.append(token_metric.semantic_entropy)\n",
        "    return answer, analysis_results, np.mean(mean_entropy), np.mean(mean_semantic_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfJHJNsZIX3S"
      },
      "outputs": [],
      "source": [
        "sample_queries = [\"What is the capital of France?\", \"What is the capital of Spain?\"]\n",
        "answer, result, mean_entropy, mean_semantic_entropy = get_vllm_response(llm, tokenizer, sampling_params, sample_queries[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEWaBBFZIX3S"
      },
      "outputs": [],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxKC5vOwIX3S"
      },
      "outputs": [],
      "source": [
        "ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate entropy metrics with a judge LLM"
      ],
      "metadata": {
        "id": "XGHwd3t10-dT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqb6tnuVIX3S"
      },
      "source": [
        "We'll create a function that uses a JudgeLLM to check if the answer predicted by the model being evaluated and the ground truth are equivalent or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WXJeuZ0Xcsk8"
      },
      "outputs": [],
      "source": [
        "def get_litellm_response(\n",
        "    text: str,\n",
        "    model: str = \"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K\",\n",
        "    top_k: int = 1\n",
        ") -> Tuple[str, str, List[str], List[float]]:\n",
        "    \"\"\"\n",
        "    Get model response and associated metrics for a given input text.\n",
        "\n",
        "    Args:\n",
        "        text: Input text to get response for\n",
        "        model: Optional model override\n",
        "        top_k: Number of top tokens to return\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - Raw model response\n",
        "        - Processed response content\n",
        "        - List of tokens\n",
        "        - List of token log probabilities\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = litellm.completion(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": text}],\n",
        "            logprobs=top_k,\n",
        "            echo=True\n",
        "        )\n",
        "        tokens = response.choices[0].logprobs.tokens\n",
        "        logprobs = response.choices[0].logprobs.token_logprobs\n",
        "        return response, response.choices[0].message.content, tokens, logprobs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {e}\")\n",
        "        return None, None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Q9O2gl25IX3S"
      },
      "outputs": [],
      "source": [
        "\n",
        "def check_answers(predicted_answer: str, correct_answer: str, judge_model: str) -> bool:\n",
        "    \"\"\"\n",
        "    A function that checks if two strings are equivalent or not by calling\n",
        "    a JudgeLLM.\n",
        "\n",
        "    Args:\n",
        "        - predicted_answer: The answer predicted by the model being evaluated.\n",
        "        - correct_answer: The ground truth answer\n",
        "        - judge_model: The judge model to be used.\n",
        "\n",
        "    Returns:\n",
        "        - A boolean 1 for equivalence.\n",
        "    \"\"\"\n",
        "    # Check for exact match first\n",
        "    if predicted_answer.strip().lower() == correct_answer.strip().lower():\n",
        "        return True\n",
        "\n",
        "    # If not an exact match, use JudgeLLM\n",
        "    prompt = f\"\"\"\n",
        "    Question: Are these two answers equivalent in meaning?\n",
        "    Answer 1: {predicted_answer}\n",
        "    Answer 2: {correct_answer}\n",
        "    Please respond with only 'yes' or 'no'.\n",
        "    \"\"\"\n",
        "\n",
        "    _, judge_response, _, _ = get_litellm_response(prompt, judge_model)\n",
        "    return judge_response.strip().lower() == 'yes'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkvjgixbIX3S"
      },
      "source": [
        "# Run model evaluation\n",
        "Let's run the model on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YyBxRwsGIX3S"
      },
      "outputs": [],
      "source": [
        "def get_halueval_predictions():\n",
        "    predicted_answers = []\n",
        "    mean_entropies = []\n",
        "    mean_semantic_entropies = []\n",
        "    correct_answers = []\n",
        "    did_hallucinate = []\n",
        "    for item in tqdm(ds):\n",
        "        correct_answers.append(item['right_answer'])\n",
        "        combined_text = f\"Context: {item['knowledge']}\\nQuestion: {item['question']}\\n Answer:\"\n",
        "        predicted_answer, result, mean_entropy, mean_semantic_entropy = get_vllm_response(llm, tokenizer, sampling_params, combined_text)\n",
        "        predicted_answers.append(predicted_answer)\n",
        "        mean_entropies.append(mean_entropy)\n",
        "        mean_semantic_entropies.append(mean_semantic_entropy)\n",
        "\n",
        "        # Check if the answers match\n",
        "        is_correct = check_answers(predicted_answer, item['right_answer'], judge_model_name)\n",
        "        did_hallucinate.append(is_correct)\n",
        "\n",
        "    return did_hallucinate, mean_entropies, mean_semantic_entropies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ywOEtD_vIX3S"
      },
      "outputs": [],
      "source": [
        "def get_truthfulqa_predictions():\n",
        "    predicted_answers = []\n",
        "    mean_entropies = []\n",
        "    mean_semantic_entropies = []\n",
        "    did_hallucinate = []\n",
        "    for ind,item in enumerate(tqdm(ds)):\n",
        "        # print(item)\n",
        "        choices = \"\\n\".join([f\"{chr(ord('A')+i)}.{choice} \" for i,choice in enumerate(item['mc1_targets']['choices'])])\n",
        "        combined_text = f\"Question: {item['question']}\\nChoices:\\n{choices}\\nAnswer:\"\n",
        "        # print(combined_text)\n",
        "        predicted_answer, result, mean_entropy, mean_semantic_entropy = get_vllm_response(llm, tokenizer, sampling_params, combined_text, 'truthful')\n",
        "        predicted_answers.append(predicted_answer)\n",
        "        mean_entropies.append(mean_entropy)\n",
        "        mean_semantic_entropies.append(mean_semantic_entropy)\n",
        "        # print(f\"Predicted answer :{predicted_answer}\")\n",
        "        correct_answer = str(chr(ord('A')+item['mc1_targets']['labels'].index(1)))\n",
        "        # print(correct_answer)\n",
        "        # Check if the answers match\n",
        "        is_correct = check_answers(str(predicted_answer), str(correct_answer), judge_model_name)\n",
        "        did_hallucinate.append(is_correct)\n",
        "\n",
        "    return did_hallucinate, mean_entropies, mean_semantic_entropies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPq8B_mcIX3S"
      },
      "outputs": [],
      "source": [
        "did_hallucinate, mean_entropies, mean_semantic_entropies = get_truthfulqa_predictions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mybJS6CjIX3S"
      },
      "outputs": [],
      "source": [
        "ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of81gADSIX3T"
      },
      "outputs": [],
      "source": [
        "len(mean_entropies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBCB3pIbIX3T"
      },
      "outputs": [],
      "source": [
        "print(f\"Model Accuracy is {np.mean(did_hallucinate)}\")\n",
        "# Hallucination when answer is wrong so let's take the inverse.\n",
        "did_hallucinate = [not x for x in did_hallucinate]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Nha0B2ceIX3T"
      },
      "outputs": [],
      "source": [
        "# Min Max Scale entropies\n",
        "scaled_mean_entropies = (np.array(mean_entropies) - np.min(mean_entropies)) / (np.max(mean_entropies) - np.min(mean_entropies))\n",
        "scaled_mean_semantic_entropies = (np.array(mean_semantic_entropies) - np.min(mean_semantic_entropies)) / (np.max(mean_semantic_entropies) - np.min(mean_semantic_entropies))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgGD5C_VIX3T"
      },
      "source": [
        "\n",
        "# ROC and PR curves for entropy metrics\n",
        "Here we plot entropy metrics and find the best threshold to detect hallucinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oJmjyjWnIX3T"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(semantic_entropy, raw_entropy, labels):\n",
        "    \"\"\"\n",
        "    Plots ROC and PR curves for semantic and raw entropy metrics.\n",
        "\n",
        "    This function calculates and visualizes the following:\n",
        "    1. ROC (Receiver Operating Characteristic) curves\n",
        "    2. PR (Precision-Recall) curves\n",
        "    3. Confusion matrices\n",
        "    4. Scatter plot of semantic vs raw entropy\n",
        "\n",
        "    For both semantic and raw entropy metrics, it computes:\n",
        "    - False Positive Rate (FPR) and True Positive Rate (TPR) for ROC curves\n",
        "    - Precision and Recall for PR curves\n",
        "    - Area Under the Curve (AUC) for both ROC and PR curves\n",
        "\n",
        "    Args:\n",
        "    semantic_entropy (array-like): Semantic entropy values\n",
        "    raw_entropy (array-like): Raw entropy values\n",
        "    labels (array-like): True labels (0 for no hallucination, 1 for hallucination)\n",
        "\n",
        "    The function creates a 1x4 subplot figure to display all plots.\n",
        "    \"\"\"\n",
        "    # Calculate metrics\n",
        "    semantic_fpr, semantic_tpr, _ = roc_curve(labels, semantic_entropy)\n",
        "    semantic_roc_auc = auc(semantic_fpr, semantic_tpr)\n",
        "    raw_fpr, raw_tpr, _ = roc_curve(labels, raw_entropy)\n",
        "    raw_roc_auc = auc(raw_fpr, raw_tpr)\n",
        "\n",
        "    semantic_precision, semantic_recall, _ = precision_recall_curve(labels, semantic_entropy)\n",
        "    raw_precision, raw_recall, _ = precision_recall_curve(labels, raw_entropy)\n",
        "\n",
        "    semantic_pr_auc = auc(semantic_recall, semantic_precision)\n",
        "    raw_pr_auc = auc(raw_recall, raw_precision)\n",
        "\n",
        "    # Plot PR curves\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', label='Random')\n",
        "    plt.plot(semantic_recall, semantic_precision, label=f'Semantic (AUC = {semantic_pr_auc:.2f})')\n",
        "    plt.plot(raw_recall, raw_precision, label=f'Raw (AUC = {raw_pr_auc:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.plot(semantic_fpr, semantic_tpr, label=f'Semantic Entropy ROC AUC = {semantic_roc_auc:.2f}')\n",
        "    plt.plot(raw_fpr, raw_tpr, label=f'Raw Entropy ROC AUC = {raw_roc_auc:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate (Recall)')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # Plot Entropy Distributions curves\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.hist(semantic_entropy, bins=50, alpha=0.5, label='Semantic')\n",
        "    plt.hist(raw_entropy, bins=50, alpha=0.5, label='Raw')\n",
        "    plt.xlabel('Entropy')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Entropy Distribution')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    thresholds = np.linspace(0, 1, 100)\n",
        "    semantic_accuracy = [accuracy_score(labels, semantic_entropy > t) for t in thresholds]\n",
        "    raw_accuracy = [accuracy_score(labels, raw_entropy > t) for t in thresholds]\n",
        "\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.plot(thresholds, semantic_accuracy, label='Semantic')\n",
        "    plt.plot(thresholds, raw_accuracy, label='Raw')\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs Threshold')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywESLtvdIX3T"
      },
      "outputs": [],
      "source": [
        "plot_metrics(scaled_mean_semantic_entropies, scaled_mean_entropies, did_hallucinate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy + Judge LLM metrics\n",
        "Here we add a powerful judge LLM and evaluate the performances in using both for hallucination detection"
      ],
      "metadata": {
        "id": "mlyydovDzeqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, roc_curve, precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from together import Together\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Initialize the Together client\n",
        "client = Together(api_key='..')\n",
        "\n",
        "def get_llm_hallucination_assessment(question, model_answer):\n",
        "    \"\"\"Uses LLama to assess hallucination probability\"\"\"\n",
        "    prompt = f\"\"\"You are an expert at detecting hallucinations in AI responses.\n",
        "    Assess if the following model answer is hallucinating (contains incorrect information).\n",
        "\n",
        "    Question: {question}\n",
        "    Model's answer: {model_answer}\n",
        "\n",
        "    Rate the hallucination on a scale from 0 to 1, where:\n",
        "    0 = No hallucination, the answer is correct\n",
        "    1 = Complete hallucination, the answer is incorrect\n",
        "\n",
        "    Return only a number between 0 and 1.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=10,\n",
        "        temperature=0.1,\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    # Parse the score\n",
        "    try:\n",
        "        import re\n",
        "        score_text = response.choices[0].message.content.strip()\n",
        "        score_match = re.search(r'(\\d+\\.\\d+|\\d+)', score_text)\n",
        "        if score_match:\n",
        "            return float(score_match.group(1))\n",
        "        else:\n",
        "            return float(score_text)\n",
        "    except:\n",
        "        return 0.5\n",
        "\n",
        "def analyze_hallucination_detection(dataset, sample_size=50):\n",
        "    \"\"\"Analyze hallucination detection methods on a dataset\"\"\"\n",
        "    # Process subset\n",
        "    subset = dataset.select(range(min(sample_size, len(dataset))))\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Process each example\n",
        "    for item in tqdm(subset):\n",
        "        question = item['question']\n",
        "        choices = \"\\n\".join([f\"{chr(ord('A')+i)}.{choice} \" for i, choice in enumerate(item['mc1_targets']['choices'])])\n",
        "        combined_text = f\"Question: {question}\\nChoices:\\n{choices}\\nAnswer:\"\n",
        "\n",
        "        # Get model response and entropy metrics\n",
        "        answer, _, raw_entropy, semantic_entropy = get_vllm_response(\n",
        "            llm, tokenizer, sampling_params, combined_text, 'truthful'\n",
        "        )\n",
        "\n",
        "        # Get correct answer\n",
        "        correct_answer = str(chr(ord('A')+item['mc1_targets']['labels'].index(1)))\n",
        "\n",
        "        # Check if answer is correct\n",
        "        is_correct = check_answers(str(answer), str(correct_answer), judge_model_name)\n",
        "        hallucinated = not is_correct\n",
        "\n",
        "        # Get model answer text\n",
        "        model_answer_text = f\"Choice {answer}: {item['mc1_targets']['choices'][ord(answer)-ord('A')] if answer.upper() in 'ABCDEFGH' else answer}\"\n",
        "\n",
        "        # Get LLM hallucination assessment\n",
        "        llm_score = get_llm_hallucination_assessment(question, model_answer_text)\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'model_answer': model_answer_text,\n",
        "            'hallucinated': hallucinated,\n",
        "            'raw_entropy': raw_entropy,\n",
        "            'semantic_entropy': semantic_entropy,\n",
        "            'llm_score': llm_score\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Normalize entropy scores (0-1 range)\n",
        "    df['entropy_norm'] = (df['semantic_entropy'] - df['semantic_entropy'].min()) / (df['semantic_entropy'].max() - df['semantic_entropy'].min())\n",
        "\n",
        "    # Calculate combined score\n",
        "    df['combined_score'] = 0.7 * df['entropy_norm'] + 0.3 * df['llm_score']\n",
        "\n",
        "    # Find optimal thresholds\n",
        "    thresholds = np.linspace(0, 1, 100)\n",
        "\n",
        "    entropy_accuracies = [accuracy_score(df['hallucinated'], df['entropy_norm'] > t) for t in thresholds]\n",
        "    best_entropy_threshold = thresholds[np.argmax(entropy_accuracies)]\n",
        "    best_entropy_accuracy = np.max(entropy_accuracies)\n",
        "\n",
        "    llm_accuracies = [accuracy_score(df['hallucinated'], df['llm_score'] > t) for t in thresholds]\n",
        "    best_llm_threshold = thresholds[np.argmax(llm_accuracies)]\n",
        "    best_llm_accuracy = np.max(llm_accuracies)\n",
        "\n",
        "    combined_accuracies = [accuracy_score(df['hallucinated'], df['combined_score'] > t) for t in thresholds]\n",
        "    best_combined_threshold = thresholds[np.argmax(combined_accuracies)]\n",
        "    best_combined_accuracy = np.max(combined_accuracies)\n",
        "\n",
        "    # Calculate correlation between entropy and LLM scores\n",
        "    correlation, p_value = pearsonr(df['entropy_norm'], df['llm_score'])\n",
        "\n",
        "    # Calculate method agreement\n",
        "    df['entropy_detect'] = df['entropy_norm'] > best_entropy_threshold\n",
        "    df['llm_detect'] = df['llm_score'] > best_llm_threshold\n",
        "\n",
        "    both_detect = df[(df['entropy_detect']) & (df['llm_detect'])].shape[0]\n",
        "    only_entropy = df[(df['entropy_detect']) & (~df['llm_detect'])].shape[0]\n",
        "    only_llm = df[(~df['entropy_detect']) & (df['llm_detect'])].shape[0]\n",
        "    neither = df[(~df['entropy_detect']) & (~df['llm_detect'])].shape[0]\n",
        "\n",
        "    # Calculate accuracy in each category\n",
        "    both_accuracy = df[(df['entropy_detect']) & (df['llm_detect'])]['hallucinated'].mean() if both_detect > 0 else 0\n",
        "    only_entropy_accuracy = df[(df['entropy_detect']) & (~df['llm_detect'])]['hallucinated'].mean() if only_entropy > 0 else 0\n",
        "    only_llm_accuracy = df[(~df['entropy_detect']) & (df['llm_detect'])]['hallucinated'].mean() if only_llm > 0 else 0\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # ROC Curves\n",
        "    plt.subplot(1, 3, 1)\n",
        "    for name, scores in [('Entropy', df['entropy_norm']), ('LLM', df['llm_score']), ('Combined', df['combined_score'])]:\n",
        "        fpr, tpr, _ = roc_curve(df['hallucinated'], scores)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], '--', color='gray')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves')\n",
        "    plt.legend()\n",
        "\n",
        "    # Score Correlation\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.scatter(\n",
        "        df['entropy_norm'],\n",
        "        df['llm_score'],\n",
        "        c=df['hallucinated'].map({True: 'red', False: 'blue'}),\n",
        "        alpha=0.7\n",
        "    )\n",
        "    plt.axhline(y=best_llm_threshold, color='gray', linestyle='--')\n",
        "    plt.axvline(x=best_entropy_threshold, color='gray', linestyle='--')\n",
        "\n",
        "    plt.xlabel('Entropy Score')\n",
        "    plt.ylabel('LLM Score')\n",
        "    plt.title(f'Correlation: {correlation:.2f}')\n",
        "    plt.legend(['Hallucination', 'Correct'])\n",
        "\n",
        "    # Score Distributions\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.hist(df[df['hallucinated']]['combined_score'], alpha=0.5, bins=10, label='Hallucination')\n",
        "    plt.hist(df[~df['hallucinated']]['combined_score'], alpha=0.5, bins=10, label='Correct')\n",
        "    plt.axvline(x=best_combined_threshold, color='r', linestyle='--')\n",
        "    plt.xlabel('Combined Score')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Score Distribution')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Best Entropy Accuracy: {best_entropy_accuracy:.2f} at threshold {best_entropy_threshold:.2f}\")\n",
        "    print(f\"Best LLM Accuracy: {best_llm_accuracy:.2f} at threshold {best_llm_threshold:.2f}\")\n",
        "    print(f\"Best Combined Accuracy: {best_combined_accuracy:.2f} at threshold {best_combined_threshold:.2f}\")\n",
        "    print(f\"Improvement from entropy to combined: {(best_combined_accuracy - best_entropy_accuracy) * 100:.1f}%\")\n",
        "\n",
        "    print(f\"\\nMethod Agreement Analysis:\")\n",
        "    print(f\"Both detect: {both_detect} examples ({both_accuracy:.1%} correct)\")\n",
        "    print(f\"Only entropy detects: {only_entropy} examples ({only_entropy_accuracy:.1%} correct)\")\n",
        "    print(f\"Only LLM detects: {only_llm} examples ({only_llm_accuracy:.1%} correct)\")\n",
        "    print(f\"Neither detects: {neither} examples\")\n",
        "\n",
        "    return df, {\n",
        "        'entropy_accuracy': best_entropy_accuracy,\n",
        "        'llm_accuracy': best_llm_accuracy,\n",
        "        'combined_accuracy': best_combined_accuracy,\n",
        "        'correlation': correlation\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load dataset\n",
        "    ds = get_trutfulqa_dataset()['validation']\n",
        "\n",
        "    # Run analysis\n",
        "    results_df, metrics = analyze_hallucination_detection(ds, sample_size=300)"
      ],
      "metadata": {
        "id": "dm1I_3F4TidY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}